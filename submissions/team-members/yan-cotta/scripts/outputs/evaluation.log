2025-05-10 07:29:24,741 - INFO - Using device: cpu
2025-05-10 07:29:24,770 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-10 07:32:27,466 - INFO - 
Processing fold 1/5
2025-05-10 07:32:27,495 - INFO - Starting epoch 1/3 for fold 1
2025-05-10 07:35:01,037 - INFO - Fold 1, Epoch 1/3, Train Loss: 0.9063
2025-05-10 07:35:01,038 - INFO - Starting epoch 2/3 for fold 1
2025-05-10 07:37:33,288 - INFO - Fold 1, Epoch 2/3, Train Loss: 0.6081
2025-05-10 07:37:33,288 - INFO - Starting epoch 3/3 for fold 1
2025-05-10 07:40:06,005 - INFO - Fold 1, Epoch 3/3, Train Loss: 0.5101
2025-05-10 07:40:06,006 - INFO - Evaluating fold 1
2025-05-10 07:40:20,568 - INFO - Fold 1, Validation Loss: 0.4551
2025-05-10 07:40:20,928 - INFO - 
Processing fold 2/5
2025-05-10 07:40:20,958 - INFO - Starting epoch 1/3 for fold 2
2025-05-10 07:42:53,818 - INFO - Fold 2, Epoch 1/3, Train Loss: 0.8690
2025-05-10 07:42:53,818 - INFO - Starting epoch 2/3 for fold 2
2025-05-10 07:45:26,306 - INFO - Fold 2, Epoch 2/3, Train Loss: 0.5804
2025-05-10 07:45:26,307 - INFO - Starting epoch 3/3 for fold 2
2025-05-10 07:47:57,674 - INFO - Fold 2, Epoch 3/3, Train Loss: 0.4921
2025-05-10 07:47:57,674 - INFO - Evaluating fold 2
2025-05-10 07:48:12,703 - INFO - Fold 2, Validation Loss: 0.4585
2025-05-10 07:48:12,914 - INFO - 
Processing fold 3/5
2025-05-10 07:48:12,944 - INFO - Starting epoch 1/3 for fold 3
2025-05-10 07:50:50,937 - INFO - Fold 3, Epoch 1/3, Train Loss: 0.9015
2025-05-10 07:50:50,937 - INFO - Starting epoch 2/3 for fold 3
2025-05-10 07:53:28,067 - INFO - Fold 3, Epoch 2/3, Train Loss: 0.5986
2025-05-10 07:53:28,067 - INFO - Starting epoch 3/3 for fold 3
2025-05-10 07:56:04,998 - INFO - Fold 3, Epoch 3/3, Train Loss: 0.5129
2025-05-10 07:56:04,998 - INFO - Evaluating fold 3
2025-05-10 07:56:20,234 - INFO - Fold 3, Validation Loss: 0.5209
2025-05-10 07:56:20,439 - INFO - 
Processing fold 4/5
2025-05-10 07:56:20,468 - INFO - Starting epoch 1/3 for fold 4
2025-05-10 07:58:55,910 - INFO - Fold 4, Epoch 1/3, Train Loss: 0.8438
2025-05-10 07:58:55,910 - INFO - Starting epoch 2/3 for fold 4
2025-05-10 08:01:28,387 - INFO - Fold 4, Epoch 2/3, Train Loss: 0.5998
2025-05-10 08:01:28,387 - INFO - Starting epoch 3/3 for fold 4
2025-05-10 08:04:04,112 - INFO - Fold 4, Epoch 3/3, Train Loss: 0.4879
2025-05-10 08:04:04,112 - INFO - Evaluating fold 4
2025-05-10 08:04:21,615 - INFO - Fold 4, Validation Loss: 0.5074
2025-05-10 08:04:21,955 - INFO - 
Processing fold 5/5
2025-05-10 08:04:21,986 - INFO - Starting epoch 1/3 for fold 5
2025-05-10 08:06:57,766 - INFO - Fold 5, Epoch 1/3, Train Loss: 0.9055
2025-05-10 08:06:57,766 - INFO - Starting epoch 2/3 for fold 5
2025-05-10 08:09:37,735 - INFO - Fold 5, Epoch 2/3, Train Loss: 0.6116
2025-05-10 08:09:37,735 - INFO - Starting epoch 3/3 for fold 5
2025-05-10 08:12:13,818 - INFO - Fold 5, Epoch 3/3, Train Loss: 0.5148
2025-05-10 08:12:13,818 - INFO - Evaluating fold 5
2025-05-10 08:12:32,654 - INFO - Fold 5, Validation Loss: 0.4932
2025-05-10 08:12:32,863 - ERROR - Error during evaluation: the resolved dtypes are not compatible with add.reduce. Resolved (dtype('<U27'), dtype('<U27'), dtype('<U54'))
2025-05-10 08:28:01,718 - INFO - Using device: cpu
2025-05-10 08:28:01,748 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-10 08:29:36,420 - INFO - 
Processing fold 1/5
2025-05-10 08:31:17,362 - INFO - Starting epoch 1/3 for fold 1
2025-05-10 08:33:55,716 - INFO - Fold 1, Epoch 1/3, Train Loss: 0.8175
2025-05-10 08:33:55,716 - INFO - Starting epoch 2/3 for fold 1
2025-05-10 08:36:28,491 - INFO - Fold 1, Epoch 2/3, Train Loss: 0.5262
2025-05-10 08:36:28,492 - INFO - Starting epoch 3/3 for fold 1
2025-05-10 08:39:01,396 - INFO - Fold 1, Epoch 3/3, Train Loss: 0.4704
2025-05-10 08:39:01,396 - INFO - Evaluating fold 1
2025-05-10 08:39:16,760 - INFO - Fold 1, Validation Loss: 2.5884
2025-05-10 08:39:17,105 - INFO - 
Processing fold 2/5
2025-05-10 08:40:28,857 - INFO - Starting epoch 1/3 for fold 2
2025-05-10 08:43:01,499 - INFO - Fold 2, Epoch 1/3, Train Loss: 0.8135
2025-05-10 08:43:01,499 - INFO - Starting epoch 2/3 for fold 2
2025-05-10 08:45:36,996 - INFO - Fold 2, Epoch 2/3, Train Loss: 0.5393
2025-05-10 08:45:36,996 - INFO - Starting epoch 3/3 for fold 2
2025-05-10 08:48:10,473 - INFO - Fold 2, Epoch 3/3, Train Loss: 0.4436
2025-05-10 08:48:10,473 - INFO - Evaluating fold 2
2025-05-10 08:48:27,931 - INFO - Fold 2, Validation Loss: 3.0854
2025-05-10 08:48:28,138 - INFO - 
Processing fold 3/5
2025-05-10 08:50:09,030 - INFO - Starting epoch 1/3 for fold 3
2025-05-10 08:52:43,703 - INFO - Fold 3, Epoch 1/3, Train Loss: 0.7631
2025-05-10 08:52:43,703 - INFO - Starting epoch 2/3 for fold 3
2025-05-10 08:55:17,777 - INFO - Fold 3, Epoch 2/3, Train Loss: 0.5075
2025-05-10 08:55:17,777 - INFO - Starting epoch 3/3 for fold 3
2025-05-10 08:57:54,842 - INFO - Fold 3, Epoch 3/3, Train Loss: 0.4449
2025-05-10 08:57:54,842 - INFO - Evaluating fold 3
2025-05-10 08:58:10,474 - INFO - Fold 3, Validation Loss: 3.0468
2025-05-10 08:58:10,694 - INFO - 
Processing fold 4/5
2025-05-10 08:59:21,653 - INFO - Starting epoch 1/3 for fold 4
2025-05-10 09:01:53,166 - INFO - Fold 4, Epoch 1/3, Train Loss: 0.7891
2025-05-10 09:01:53,166 - INFO - Starting epoch 2/3 for fold 4
2025-05-10 09:04:27,413 - INFO - Fold 4, Epoch 2/3, Train Loss: 0.5076
2025-05-10 09:04:27,413 - INFO - Starting epoch 3/3 for fold 4
2025-05-10 09:07:07,805 - INFO - Fold 4, Epoch 3/3, Train Loss: 0.4568
2025-05-10 09:07:07,805 - INFO - Evaluating fold 4
2025-05-10 09:07:26,240 - INFO - Fold 4, Validation Loss: 2.6323
2025-05-10 09:07:26,451 - INFO - 
Processing fold 5/5
2025-05-10 09:09:06,519 - INFO - Starting epoch 1/3 for fold 5
2025-05-10 09:11:38,169 - INFO - Fold 5, Epoch 1/3, Train Loss: 0.7418
2025-05-10 09:11:38,169 - INFO - Starting epoch 2/3 for fold 5
2025-05-10 09:14:06,372 - INFO - Fold 5, Epoch 2/3, Train Loss: 0.5113
2025-05-10 09:14:06,372 - INFO - Starting epoch 3/3 for fold 5
2025-05-10 09:16:38,428 - INFO - Fold 5, Epoch 3/3, Train Loss: 0.4482
2025-05-10 09:16:38,428 - INFO - Evaluating fold 5
2025-05-10 09:16:57,097 - INFO - Fold 5, Validation Loss: 3.0201
2025-05-10 09:16:57,448 - ERROR - Error during evaluation: name 'json' is not defined
2025-05-10 09:20:56,794 - INFO - Using device: cpu
2025-05-10 09:20:56,824 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-10 09:22:31,047 - INFO - 
Processing fold 1/5
2025-05-10 09:22:31,075 - INFO - Starting epoch 1/3 for fold 1
2025-05-10 09:25:08,471 - INFO - Fold 1, Epoch 1/3, Train Loss: 0.6709
2025-05-10 09:25:08,471 - INFO - Starting epoch 2/3 for fold 1
2025-05-10 09:27:42,464 - INFO - Fold 1, Epoch 2/3, Train Loss: 0.4358
2025-05-10 09:27:42,464 - INFO - Starting epoch 3/3 for fold 1
2025-05-10 09:30:16,823 - INFO - Fold 1, Epoch 3/3, Train Loss: 0.3899
2025-05-10 09:30:16,823 - INFO - Evaluating fold 1
2025-05-10 09:30:32,069 - INFO - Fold 1, Validation Loss: 2.6507
2025-05-10 09:30:32,400 - INFO - 
Processing fold 2/5
2025-05-10 09:30:32,431 - INFO - Starting epoch 1/3 for fold 2
2025-05-10 09:33:06,202 - INFO - Fold 2, Epoch 1/3, Train Loss: 0.6471
2025-05-10 09:33:06,202 - INFO - Starting epoch 2/3 for fold 2
2025-05-10 09:35:43,002 - INFO - Fold 2, Epoch 2/3, Train Loss: 0.4362
2025-05-10 09:35:43,002 - INFO - Starting epoch 3/3 for fold 2
2025-05-10 09:38:21,986 - INFO - Fold 2, Epoch 3/3, Train Loss: 0.3617
2025-05-10 09:38:21,986 - INFO - Evaluating fold 2
2025-05-10 09:38:39,782 - INFO - Fold 2, Validation Loss: 2.5724
2025-05-10 09:38:39,988 - INFO - 
Processing fold 3/5
2025-05-10 09:38:40,019 - INFO - Starting epoch 1/3 for fold 3
2025-05-10 09:41:15,797 - INFO - Fold 3, Epoch 1/3, Train Loss: 0.6399
2025-05-10 09:41:15,797 - INFO - Starting epoch 2/3 for fold 3
2025-05-10 09:43:52,492 - INFO - Fold 3, Epoch 2/3, Train Loss: 0.4313
2025-05-10 09:43:52,492 - INFO - Starting epoch 3/3 for fold 3
2025-05-10 09:46:29,614 - INFO - Fold 3, Epoch 3/3, Train Loss: 0.3764
2025-05-10 09:46:29,614 - INFO - Evaluating fold 3
2025-05-10 09:46:44,899 - INFO - Fold 3, Validation Loss: 2.9730
2025-05-10 09:46:45,104 - INFO - 
Processing fold 4/5
2025-05-10 09:46:45,135 - INFO - Starting epoch 1/3 for fold 4
2025-05-10 09:49:23,346 - INFO - Fold 4, Epoch 1/3, Train Loss: 0.6578
2025-05-10 09:49:23,346 - INFO - Starting epoch 2/3 for fold 4
2025-05-10 09:51:58,937 - INFO - Fold 4, Epoch 2/3, Train Loss: 0.4370
2025-05-10 09:51:58,937 - INFO - Starting epoch 3/3 for fold 4
2025-05-10 09:54:36,104 - INFO - Fold 4, Epoch 3/3, Train Loss: 0.3715
2025-05-10 09:54:36,104 - INFO - Evaluating fold 4
2025-05-10 09:54:51,679 - INFO - Fold 4, Validation Loss: 2.7427
2025-05-10 09:54:51,880 - INFO - 
Processing fold 5/5
2025-05-10 09:54:51,910 - INFO - Starting epoch 1/3 for fold 5
2025-05-10 09:57:30,392 - INFO - Fold 5, Epoch 1/3, Train Loss: 0.6397
2025-05-10 09:57:30,392 - INFO - Starting epoch 2/3 for fold 5
2025-05-10 10:00:07,654 - INFO - Fold 5, Epoch 2/3, Train Loss: 0.4249
2025-05-10 10:00:07,654 - INFO - Starting epoch 3/3 for fold 5
2025-05-10 10:02:47,497 - INFO - Fold 5, Epoch 3/3, Train Loss: 0.3761
2025-05-10 10:02:47,497 - INFO - Evaluating fold 5
2025-05-10 10:03:05,805 - INFO - Fold 5, Validation Loss: 2.9557
2025-05-10 10:03:06,296 - INFO - 
Evaluation with oversampling completed successfully!
2025-05-10 10:17:21,670 - INFO - Using device: cpu
2025-05-10 10:17:21,700 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-10 10:19:17,415 - INFO - 
Testing parameters: {'lr': 0.01, 'dropout': 0.7, 'batch_size': 16}
2025-05-10 10:19:17,504 - INFO - Processing fold 1/5
2025-05-10 10:22:05,979 - INFO - Fold 1, Epoch 1/5, Train Loss: 1.4831
2025-05-10 10:24:50,935 - INFO - Fold 1, Epoch 2/5, Train Loss: 0.8383
2025-05-10 10:27:34,949 - INFO - Fold 1, Epoch 3/5, Train Loss: 0.6690
2025-05-10 10:30:18,545 - INFO - Fold 1, Epoch 4/5, Train Loss: 0.5825
2025-05-10 10:33:02,680 - INFO - Fold 1, Epoch 5/5, Train Loss: 0.5364
2025-05-10 10:33:19,049 - INFO - Processing fold 2/5
2025-05-10 10:36:08,743 - INFO - Fold 2, Epoch 1/5, Train Loss: 1.2691
2025-05-10 10:38:54,748 - INFO - Fold 2, Epoch 2/5, Train Loss: 0.7721
2025-05-10 10:41:40,340 - INFO - Fold 2, Epoch 3/5, Train Loss: 0.6539
2025-05-10 10:44:26,229 - INFO - Fold 2, Epoch 4/5, Train Loss: 0.5990
2025-05-10 10:47:11,429 - INFO - Fold 2, Epoch 5/5, Train Loss: 0.5319
2025-05-10 10:47:27,514 - INFO - Processing fold 3/5
2025-05-10 10:50:12,942 - INFO - Fold 3, Epoch 1/5, Train Loss: 1.5404
2025-05-10 10:52:58,517 - INFO - Fold 3, Epoch 2/5, Train Loss: 0.9098
2025-05-10 10:55:45,064 - INFO - Fold 3, Epoch 3/5, Train Loss: 0.6928
2025-05-10 10:58:30,536 - INFO - Fold 3, Epoch 4/5, Train Loss: 0.6014
2025-05-10 11:01:16,149 - INFO - Fold 3, Epoch 5/5, Train Loss: 0.5618
2025-05-10 11:01:32,043 - INFO - Processing fold 4/5
2025-05-10 11:04:14,913 - INFO - Fold 4, Epoch 1/5, Train Loss: 1.3080
2025-05-10 11:06:57,087 - INFO - Fold 4, Epoch 2/5, Train Loss: 0.8377
2025-05-10 11:09:41,903 - INFO - Fold 4, Epoch 3/5, Train Loss: 0.6961
2025-05-10 11:12:25,125 - INFO - Fold 4, Epoch 4/5, Train Loss: 0.5991
2025-05-10 11:15:08,708 - INFO - Fold 4, Epoch 5/5, Train Loss: 0.5339
2025-05-10 11:15:24,831 - INFO - Processing fold 5/5
2025-05-10 11:18:13,156 - INFO - Fold 5, Epoch 1/5, Train Loss: 1.3942
2025-05-10 11:20:57,540 - INFO - Fold 5, Epoch 2/5, Train Loss: 0.7990
2025-05-10 11:23:42,019 - INFO - Fold 5, Epoch 3/5, Train Loss: 0.6411
2025-05-10 11:26:25,267 - INFO - Fold 5, Epoch 4/5, Train Loss: 0.5726
2025-05-10 11:29:11,289 - INFO - Fold 5, Epoch 5/5, Train Loss: 0.5249
2025-05-10 11:29:27,364 - INFO - Average F1 score for parameters {'lr': 0.01, 'dropout': 0.7, 'batch_size': 16}: 0.5601
2025-05-10 11:29:27,365 - INFO - 
Testing parameters: {'lr': 0.001, 'dropout': 0.5, 'batch_size': 32}
2025-05-10 11:29:27,453 - INFO - Processing fold 1/5
2025-05-10 11:32:17,783 - INFO - Fold 1, Epoch 1/5, Train Loss: 0.7574
2025-05-10 11:35:08,927 - INFO - Fold 1, Epoch 2/5, Train Loss: 0.4719
2025-05-10 11:37:57,090 - INFO - Fold 1, Epoch 3/5, Train Loss: 0.4122
2025-05-10 11:40:44,161 - INFO - Fold 1, Epoch 4/5, Train Loss: 0.3603
2025-05-10 11:43:29,700 - INFO - Fold 1, Epoch 5/5, Train Loss: 0.3435
2025-05-10 11:43:46,105 - INFO - Processing fold 2/5
2025-05-10 11:46:34,054 - INFO - Fold 2, Epoch 1/5, Train Loss: 0.6863
2025-05-10 11:49:22,786 - INFO - Fold 2, Epoch 2/5, Train Loss: 0.4494
2025-05-10 11:52:08,268 - INFO - Fold 2, Epoch 3/5, Train Loss: 0.4153
2025-05-10 11:54:54,455 - INFO - Fold 2, Epoch 4/5, Train Loss: 0.3470
2025-05-10 11:57:41,516 - INFO - Fold 2, Epoch 5/5, Train Loss: 0.3334
2025-05-10 11:57:57,985 - INFO - Processing fold 3/5
2025-05-10 12:00:43,303 - INFO - Fold 3, Epoch 1/5, Train Loss: 0.7378
2025-05-10 12:03:28,733 - INFO - Fold 3, Epoch 2/5, Train Loss: 0.4687
2025-05-10 12:06:13,273 - INFO - Fold 3, Epoch 3/5, Train Loss: 0.4039
2025-05-10 12:08:59,558 - INFO - Fold 3, Epoch 4/5, Train Loss: 0.3717
2025-05-10 12:11:45,363 - INFO - Fold 3, Epoch 5/5, Train Loss: 0.3339
2025-05-10 12:12:02,079 - INFO - Processing fold 4/5
2025-05-10 12:14:46,750 - INFO - Fold 4, Epoch 1/5, Train Loss: 0.6917
2025-05-10 12:17:32,793 - INFO - Fold 4, Epoch 2/5, Train Loss: 0.4590
2025-05-10 12:20:17,676 - INFO - Fold 4, Epoch 3/5, Train Loss: 0.4167
2025-05-10 12:23:03,105 - INFO - Fold 4, Epoch 4/5, Train Loss: 0.3553
2025-05-10 12:25:48,419 - INFO - Fold 4, Epoch 5/5, Train Loss: 0.3352
2025-05-10 12:26:05,061 - INFO - Processing fold 5/5
2025-05-10 12:28:52,824 - INFO - Fold 5, Epoch 1/5, Train Loss: 0.7253
2025-05-10 12:31:34,189 - INFO - Fold 5, Epoch 2/5, Train Loss: 0.4911
2025-05-10 12:34:14,883 - INFO - Fold 5, Epoch 3/5, Train Loss: 0.4247
2025-05-10 12:36:56,175 - INFO - Fold 5, Epoch 4/5, Train Loss: 0.3877
2025-05-10 12:39:39,147 - INFO - Fold 5, Epoch 5/5, Train Loss: 0.3451
2025-05-10 12:39:57,234 - INFO - Average F1 score for parameters {'lr': 0.001, 'dropout': 0.5, 'batch_size': 32}: 0.6333
2025-05-10 12:39:57,234 - INFO - 
Testing parameters: {'lr': 0.0001, 'dropout': 0.3, 'batch_size': 32}
2025-05-10 12:39:57,325 - INFO - Processing fold 1/5
2025-05-10 12:46:20,033 - INFO - Using device: cpu
2025-05-10 12:46:20,063 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-10 12:48:13,946 - INFO - Class weights: tensor([0.7804, 1.8134, 0.8006, 0.9445, 0.9303, 6.1203, 0.9303, 1.5176, 0.6252,
        0.9522, 0.9303, 1.0314, 0.8336, 1.0068])
2025-05-10 12:50:16,650 - INFO - 
Processing fold 1/5
2025-05-10 12:53:08,480 - INFO - Fold 1, Epoch 1/10, Train Loss: 0.8564
2025-05-10 12:55:58,222 - INFO - Fold 1, Epoch 2/10, Train Loss: 0.5528
2025-05-10 12:58:46,082 - INFO - Fold 1, Epoch 3/10, Train Loss: 0.4582
2025-05-10 13:01:35,327 - INFO - Fold 1, Epoch 4/10, Train Loss: 0.4601
2025-05-10 13:04:23,426 - INFO - Fold 1, Epoch 5/10, Train Loss: 0.4077
2025-05-10 13:07:11,135 - INFO - Fold 1, Epoch 6/10, Train Loss: 0.3722
2025-05-10 13:09:59,574 - INFO - Fold 1, Epoch 7/10, Train Loss: 0.3427
2025-05-10 13:12:47,220 - INFO - Fold 1, Epoch 8/10, Train Loss: 0.3512
2025-05-10 13:15:35,673 - INFO - Fold 1, Epoch 9/10, Train Loss: 0.3275
2025-05-10 13:18:22,673 - INFO - Fold 1, Epoch 10/10, Train Loss: 0.2971
2025-05-10 13:18:39,498 - INFO - 
Processing fold 2/5
2025-05-10 13:21:27,049 - INFO - Fold 2, Epoch 1/10, Train Loss: 0.8532
2025-05-10 13:24:15,603 - INFO - Fold 2, Epoch 2/10, Train Loss: 0.5573
2025-05-10 13:27:04,967 - INFO - Fold 2, Epoch 3/10, Train Loss: 0.4693
2025-05-10 13:29:56,913 - INFO - Fold 2, Epoch 4/10, Train Loss: 0.4405
2025-05-10 13:32:46,703 - INFO - Fold 2, Epoch 5/10, Train Loss: 0.4058
2025-05-10 13:35:34,795 - INFO - Fold 2, Epoch 6/10, Train Loss: 0.3485
2025-05-10 13:38:20,058 - INFO - Fold 2, Epoch 7/10, Train Loss: 0.3450
2025-05-10 13:41:04,819 - INFO - Fold 2, Epoch 8/10, Train Loss: 0.3657
2025-05-10 13:43:53,578 - INFO - Fold 2, Epoch 9/10, Train Loss: 0.3154
2025-05-10 13:46:41,990 - INFO - Fold 2, Epoch 10/10, Train Loss: 0.3103
2025-05-10 13:46:58,929 - INFO - 
Processing fold 3/5
2025-05-10 13:49:47,898 - INFO - Fold 3, Epoch 1/10, Train Loss: 0.8933
2025-05-10 13:52:37,358 - INFO - Fold 3, Epoch 2/10, Train Loss: 0.5739
2025-05-10 13:55:24,836 - INFO - Fold 3, Epoch 3/10, Train Loss: 0.4813
2025-05-10 13:58:11,632 - INFO - Fold 3, Epoch 4/10, Train Loss: 0.4245
2025-05-10 14:01:00,786 - INFO - Fold 3, Epoch 5/10, Train Loss: 0.4211
2025-05-10 14:03:48,800 - INFO - Fold 3, Epoch 6/10, Train Loss: 0.3767
2025-05-10 14:06:36,246 - INFO - Fold 3, Epoch 7/10, Train Loss: 0.3607
2025-05-10 14:09:24,373 - INFO - Fold 3, Epoch 8/10, Train Loss: 0.3431
2025-05-10 14:12:12,913 - INFO - Fold 3, Epoch 9/10, Train Loss: 0.3547
2025-05-10 14:15:00,698 - INFO - Fold 3, Epoch 10/10, Train Loss: 0.3264
2025-05-10 14:15:19,003 - INFO - 
Processing fold 4/5
2025-05-10 14:18:07,775 - INFO - Fold 4, Epoch 1/10, Train Loss: 0.8845
2025-05-10 14:20:56,825 - INFO - Fold 4, Epoch 2/10, Train Loss: 0.5616
2025-05-10 14:23:44,700 - INFO - Fold 4, Epoch 3/10, Train Loss: 0.4798
2025-05-10 14:26:32,732 - INFO - Fold 4, Epoch 4/10, Train Loss: 0.4444
2025-05-10 14:29:20,543 - INFO - Fold 4, Epoch 5/10, Train Loss: 0.3993
2025-05-10 14:32:10,201 - INFO - Fold 4, Epoch 6/10, Train Loss: 0.3883
2025-05-10 14:34:59,846 - INFO - Fold 4, Epoch 7/10, Train Loss: 0.3491
2025-05-10 14:37:49,549 - INFO - Fold 4, Epoch 8/10, Train Loss: 0.3409
2025-05-10 14:40:37,652 - INFO - Fold 4, Epoch 9/10, Train Loss: 0.3409
2025-05-10 14:43:24,781 - INFO - Fold 4, Epoch 10/10, Train Loss: 0.3171
2025-05-10 14:43:41,476 - INFO - 
Processing fold 5/5
2025-05-10 14:46:31,313 - INFO - Fold 5, Epoch 1/10, Train Loss: 0.8613
2025-05-10 14:49:22,837 - INFO - Fold 5, Epoch 2/10, Train Loss: 0.5723
2025-05-10 14:52:10,745 - INFO - Fold 5, Epoch 3/10, Train Loss: 0.4822
2025-05-10 14:54:59,630 - INFO - Fold 5, Epoch 4/10, Train Loss: 0.4371
2025-05-10 14:57:45,461 - INFO - Fold 5, Epoch 5/10, Train Loss: 0.3980
2025-05-10 15:00:32,901 - INFO - Fold 5, Epoch 6/10, Train Loss: 0.3706
2025-05-10 15:03:20,361 - INFO - Fold 5, Epoch 7/10, Train Loss: 0.3531
2025-05-10 15:06:07,853 - INFO - Fold 5, Epoch 8/10, Train Loss: 0.3485
2025-05-10 15:08:54,934 - INFO - Fold 5, Epoch 9/10, Train Loss: 0.3353
2025-05-10 15:11:41,355 - INFO - Fold 5, Epoch 10/10, Train Loss: 0.3315
2025-05-10 15:11:56,950 - INFO - 
Evaluation completed successfully!
2025-05-11 13:15:38,367 - INFO - Using device: cuda
2025-05-11 13:15:38,368 - INFO - GPU Name: AMD Radeon Graphics
2025-05-11 13:15:38,398 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-11 13:17:47,268 - ERROR - Error during evaluation: HIP error: invalid device function
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.

2025-05-11 13:19:38,941 - INFO - Using device: cuda
2025-05-11 13:19:38,942 - INFO - GPU Name: AMD Radeon Graphics
2025-05-11 13:19:38,972 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-11 13:21:51,858 - ERROR - Error during evaluation: HIP error: invalid device function
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.

2025-05-11 13:40:39,379 - INFO - Using device: cuda
2025-05-11 13:40:39,380 - INFO - GPU Name: AMD Radeon Graphics
2025-05-11 13:40:39,409 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-11 13:42:56,840 - ERROR - Error during evaluation: HIP error: invalid device function
HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing AMD_SERIALIZE_KERNEL=3
Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.

2025-05-14 15:38:50,609 - INFO - Using device: cpu
2025-05-14 15:38:50,640 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-14 15:41:04,177 - INFO - Class weights: tensor([0.7804, 1.8134, 0.8006, 0.9445, 0.9303, 6.1203, 0.9303, 1.5176, 0.6252,
        0.9522, 0.9303, 1.0314, 0.8336, 1.0068])
2025-05-14 15:45:18,242 - INFO - 
Processing fold 1/5
2025-05-14 15:45:21,253 - INFO - Applying SMOTE oversampling to training data...
2025-05-14 15:46:56,120 - INFO - Using device: cpu
2025-05-14 15:46:56,151 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-14 15:49:18,168 - INFO - Class weights: tensor([0.7804, 1.8134, 0.8006, 0.9445, 0.9303, 6.1203, 0.9303, 1.5176, 0.6252,
        0.9522, 0.9303, 1.0314, 0.8336, 1.0068])
2025-05-14 15:54:00,583 - INFO - 
Processing fold 1/5
2025-05-14 15:54:02,745 - INFO - Applying SMOTE oversampling to training data...
2025-05-16 15:44:21,630 - INFO - Using device: cpu
2025-05-16 15:44:21,662 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-16 15:46:33,388 - INFO - Class weights: tensor([0.7804, 1.8134, 0.8006, 0.9445, 0.9303, 6.1203, 0.9303, 1.5176, 0.6252,
        0.9522, 0.9303, 1.0314, 0.8336, 1.0068])
2025-05-16 15:48:28,409 - INFO - 
Processing fold 1/5
2025-05-16 15:51:26,964 - INFO - Fold 1, Epoch 1/10, Train Loss: 1.0437, Val Loss: 0.8749
2025-05-16 15:54:26,099 - INFO - Fold 1, Epoch 2/10, Train Loss: 0.7337, Val Loss: 0.8232
2025-05-16 15:57:25,758 - INFO - Fold 1, Epoch 3/10, Train Loss: 0.6329, Val Loss: 0.6065
2025-05-16 16:00:25,760 - INFO - Fold 1, Epoch 4/10, Train Loss: 0.5890, Val Loss: 0.4720
2025-05-16 16:03:26,864 - INFO - Fold 1, Epoch 5/10, Train Loss: 0.5442, Val Loss: 1.0182
2025-05-16 16:03:26,864 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 16:06:27,481 - INFO - Fold 1, Epoch 6/10, Train Loss: 0.5415, Val Loss: 0.4243
2025-05-16 16:09:28,322 - INFO - Fold 1, Epoch 7/10, Train Loss: 0.5006, Val Loss: 0.3937
2025-05-16 16:12:29,612 - INFO - Fold 1, Epoch 8/10, Train Loss: 0.5033, Val Loss: 0.8113
2025-05-16 16:12:29,612 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 16:15:30,870 - INFO - Fold 1, Epoch 9/10, Train Loss: 0.4628, Val Loss: 0.8758
2025-05-16 16:15:30,870 - INFO - EarlyStopping counter: 2 out of 3
2025-05-16 16:18:31,967 - INFO - Fold 1, Epoch 10/10, Train Loss: 0.4482, Val Loss: 0.3968
2025-05-16 16:18:31,967 - INFO - EarlyStopping counter: 3 out of 3
2025-05-16 16:18:31,967 - INFO - Early stopping triggered at epoch 10
2025-05-16 16:18:48,844 - INFO - 
Processing fold 2/5
2025-05-16 16:21:49,738 - INFO - Fold 2, Epoch 1/10, Train Loss: 1.0685, Val Loss: 0.7334
2025-05-16 16:24:51,123 - INFO - Fold 2, Epoch 2/10, Train Loss: 0.7309, Val Loss: 0.7978
2025-05-16 16:24:51,123 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 16:27:52,198 - INFO - Fold 2, Epoch 3/10, Train Loss: 0.6538, Val Loss: 0.5928
2025-05-16 16:30:53,798 - INFO - Fold 2, Epoch 4/10, Train Loss: 0.5976, Val Loss: 0.6822
2025-05-16 16:30:53,798 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 16:33:55,102 - INFO - Fold 2, Epoch 5/10, Train Loss: 0.5424, Val Loss: 0.4794
2025-05-16 16:36:55,899 - INFO - Fold 2, Epoch 6/10, Train Loss: 0.5111, Val Loss: 0.4742
2025-05-16 16:39:56,858 - INFO - Fold 2, Epoch 7/10, Train Loss: 0.5034, Val Loss: 0.4859
2025-05-16 16:39:56,858 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 16:42:56,745 - INFO - Fold 2, Epoch 8/10, Train Loss: 0.4676, Val Loss: 0.3906
2025-05-16 16:45:58,004 - INFO - Fold 2, Epoch 9/10, Train Loss: 0.4793, Val Loss: 0.5471
2025-05-16 16:45:58,004 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 16:48:59,404 - INFO - Fold 2, Epoch 10/10, Train Loss: 0.4327, Val Loss: 0.4006
2025-05-16 16:48:59,404 - INFO - EarlyStopping counter: 2 out of 3
2025-05-16 16:49:15,942 - INFO - 
Processing fold 3/5
2025-05-16 16:52:17,422 - INFO - Fold 3, Epoch 1/10, Train Loss: 1.0578, Val Loss: 0.7636
2025-05-16 16:55:18,185 - INFO - Fold 3, Epoch 2/10, Train Loss: 0.7401, Val Loss: 0.7094
2025-05-16 16:58:19,159 - INFO - Fold 3, Epoch 3/10, Train Loss: 0.6549, Val Loss: 0.7575
2025-05-16 16:58:19,159 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 17:01:20,441 - INFO - Fold 3, Epoch 4/10, Train Loss: 0.6138, Val Loss: 0.6865
2025-05-16 17:04:20,845 - INFO - Fold 3, Epoch 5/10, Train Loss: 0.5510, Val Loss: 0.5081
2025-05-16 17:07:21,828 - INFO - Fold 3, Epoch 6/10, Train Loss: 0.5074, Val Loss: 0.4790
2025-05-16 17:10:22,742 - INFO - Fold 3, Epoch 7/10, Train Loss: 0.5055, Val Loss: 0.5665
2025-05-16 17:10:22,742 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 17:13:24,347 - INFO - Fold 3, Epoch 8/10, Train Loss: 0.4789, Val Loss: 0.3926
2025-05-16 17:16:25,233 - INFO - Fold 3, Epoch 9/10, Train Loss: 0.4577, Val Loss: 0.4281
2025-05-16 17:16:25,233 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 17:19:25,664 - INFO - Fold 3, Epoch 10/10, Train Loss: 0.4735, Val Loss: 0.5650
2025-05-16 17:19:25,664 - INFO - EarlyStopping counter: 2 out of 3
2025-05-16 17:19:42,276 - INFO - 
Processing fold 4/5
2025-05-16 17:22:43,908 - INFO - Fold 4, Epoch 1/10, Train Loss: 1.0530, Val Loss: 0.7687
2025-05-16 17:25:45,141 - INFO - Fold 4, Epoch 2/10, Train Loss: 0.7250, Val Loss: 0.7103
2025-05-16 17:28:43,101 - INFO - Fold 4, Epoch 3/10, Train Loss: 0.6396, Val Loss: 0.5288
2025-05-16 17:31:41,844 - INFO - Fold 4, Epoch 4/10, Train Loss: 0.5958, Val Loss: 0.5654
2025-05-16 17:31:41,844 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 17:34:39,500 - INFO - Fold 4, Epoch 5/10, Train Loss: 0.5804, Val Loss: 0.4867
2025-05-16 17:37:38,613 - INFO - Fold 4, Epoch 6/10, Train Loss: 0.5299, Val Loss: 0.4619
2025-05-16 17:40:36,118 - INFO - Fold 4, Epoch 7/10, Train Loss: 0.5179, Val Loss: 0.4923
2025-05-16 17:40:36,118 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 17:43:35,286 - INFO - Fold 4, Epoch 8/10, Train Loss: 0.4898, Val Loss: 1.4627
2025-05-16 17:43:35,286 - INFO - EarlyStopping counter: 2 out of 3
2025-05-16 17:46:33,220 - INFO - Fold 4, Epoch 9/10, Train Loss: 0.4814, Val Loss: 0.5132
2025-05-16 17:46:33,220 - INFO - EarlyStopping counter: 3 out of 3
2025-05-16 17:46:33,220 - INFO - Early stopping triggered at epoch 9
2025-05-16 17:46:49,363 - INFO - 
Processing fold 5/5
2025-05-16 17:49:46,413 - INFO - Fold 5, Epoch 1/10, Train Loss: 1.0166, Val Loss: 0.7213
2025-05-16 17:52:46,993 - INFO - Fold 5, Epoch 2/10, Train Loss: 0.7343, Val Loss: 0.5996
2025-05-16 17:55:44,339 - INFO - Fold 5, Epoch 3/10, Train Loss: 0.6390, Val Loss: 0.5223
2025-05-16 17:58:45,344 - INFO - Fold 5, Epoch 4/10, Train Loss: 0.5932, Val Loss: 0.6432
2025-05-16 17:58:45,344 - INFO - EarlyStopping counter: 1 out of 3
2025-05-16 18:01:43,207 - INFO - Fold 5, Epoch 5/10, Train Loss: 0.5630, Val Loss: 0.6069
2025-05-16 18:01:43,208 - INFO - EarlyStopping counter: 2 out of 3
2025-05-16 18:04:42,133 - INFO - Fold 5, Epoch 6/10, Train Loss: 0.5258, Val Loss: 0.8772
2025-05-16 18:04:42,133 - INFO - EarlyStopping counter: 3 out of 3
2025-05-16 18:04:42,133 - INFO - Early stopping triggered at epoch 6
2025-05-16 18:04:59,357 - INFO - 
Evaluation completed successfully!
2025-05-19 10:57:07,141 - INFO - Using device: cpu
2025-05-19 10:57:07,142 - ERROR - Error during evaluation: get_transforms() got an unexpected keyword argument 'img_size'
2025-05-19 10:58:47,253 - INFO - Using device: cpu
2025-05-19 10:58:47,282 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-19 10:58:47,283 - ERROR - Error during evaluation: 'ImageFolder' object has no attribute 'float'
2025-05-19 11:03:02,600 - INFO - Using device: cpu
2025-05-19 11:03:02,629 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-19 11:05:09,894 - INFO - Class weights: tensor([0.8034, 1.2247, 0.8137, 0.8838, 0.8771, 2.2498, 0.8771, 1.1203, 0.7191,
        0.8874, 0.8771, 0.9236, 0.8303, 0.9125])
2025-05-19 11:07:12,626 - INFO - 
Processing fold 1/5
2025-05-19 11:09:00,235 - INFO - Applying SMOTE oversampling to training data...
2025-05-19 11:09:00,444 - ERROR - Error during SMOTE: With over-sampling methods, the number of samples in a class should be greater or equal to the original number of samples. Originally, there is 1191 samples and 500 samples are asked.
2025-05-19 11:09:00,444 - WARNING - Proceeding with original data due to SMOTE error
2025-05-19 11:09:05,042 - ERROR - Error during evaluation: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'
2025-05-19 11:13:16,807 - INFO - Using device: cpu
2025-05-19 11:13:16,836 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-19 11:15:28,998 - INFO - Class weights: tensor([0.8034, 1.2247, 0.8137, 0.8838, 0.8771, 2.2498, 0.8771, 1.1203, 0.7191,
        0.8874, 0.8771, 0.9236, 0.8303, 0.9125])
2025-05-19 11:17:30,443 - INFO - 
Processing fold 1/5
2025-05-19 11:19:17,920 - INFO - Applying SMOTE oversampling to training data...
2025-05-19 11:19:17,920 - INFO - Original class counts: [ 953  411  929  788  800  122  800  490 1191  781  800  722  893  739]
2025-05-19 11:19:17,920 - INFO - SMOTE sampling strategy: {8: 1786, 9: 1171, 5: 500}
2025-05-19 11:19:37,256 - INFO - SMOTE completed. New sample distribution: [ 953  411  929  788  800  500  800  490 1786 1171  800  722  893  739]
2025-05-19 11:19:37,858 - ERROR - Error during evaluation: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/scripts/model_evaluation.py", line 133, in __getitem__
    feature = self.transform(feature)
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py", line 95, in __call__
    img = t(img)
          ^^^^^^
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py", line 137, in __call__
    return F.to_tensor(pic)
           ^^^^^^^^^^^^^^^^
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/torchvision/transforms/functional.py", line 142, in to_tensor
    raise TypeError(f"pic should be PIL Image or ndarray. Got {type(pic)}")
TypeError: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>

2025-05-19 12:13:59,026 - INFO - Using device: cpu
2025-05-19 12:13:59,067 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-19 12:16:17,441 - INFO - Class weights: tensor([0.8034, 1.2247, 0.8137, 0.8838, 0.8771, 2.2498, 0.8771, 1.1203, 0.7191,
        0.8874, 0.8771, 0.9236, 0.8303, 0.9125])
2025-05-19 12:18:28,498 - INFO - 
Processing fold 1/5
2025-05-19 12:20:14,727 - INFO - Applying SMOTE oversampling to training data...
2025-05-19 12:20:14,728 - INFO - Original class counts: [ 953  411  929  788  800  122  800  490 1191  781  800  722  893  739]
2025-05-19 12:20:14,728 - INFO - SMOTE sampling strategy: {8: 1786, 9: 1171, 5: 500}
2025-05-19 12:20:22,409 - INFO - SMOTE completed. New sample distribution: [ 953  411  929  788  800  500  800  490 1786 1171  800  722  893  739]
2025-05-19 12:20:22,765 - ERROR - Error during evaluation: Caught NameError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/scripts/model_evaluation.py", line 134, in __getitem__
    feature = Image.fromarray((feature * 255).astype(np.uint8))  # Convert to PIL Image
              ^^^^^
NameError: name 'Image' is not defined

2025-05-19 12:24:04,667 - INFO - Using device: cpu
2025-05-19 12:24:04,696 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-19 12:26:20,559 - INFO - Class weights: tensor([0.8034, 1.2247, 0.8137, 0.8838, 0.8771, 2.2498, 0.8771, 1.1203, 0.7191,
        0.8874, 0.8771, 0.9236, 0.8303, 0.9125])
2025-05-19 12:28:30,865 - INFO - 
Processing fold 1/5
2025-05-19 12:30:19,089 - INFO - Applying SMOTE oversampling to training data...
2025-05-19 12:30:19,089 - INFO - Original class counts: [ 953  411  929  788  800  122  800  490 1191  781  800  722  893  739]
2025-05-19 12:30:19,089 - INFO - SMOTE sampling strategy: {8: 1786, 9: 1171, 5: 500}
2025-05-19 12:30:26,481 - INFO - SMOTE completed. New sample distribution: [ 953  411  929  788  800  500  800  490 1786 1171  800  722  893  739]
2025-05-19 12:30:26,821 - ERROR - Error during evaluation: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/PIL/Image.py", line 3315, in fromarray
    mode, rawmode = _fromarray_typemap[typekey]
                    ~~~~~~~~~~~~~~~~~~^^^^^^^^^
KeyError: ((1, 1, 224), '|u1')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/scripts/model_evaluation.py", line 135, in __getitem__
    feature = Image.fromarray((feature * 255).astype(np.uint8))  # Convert to PIL Image
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/venv/lib/python3.12/site-packages/PIL/Image.py", line 3319, in fromarray
    raise TypeError(msg) from e
TypeError: Cannot handle this data type: (1, 1, 224), |u1

2025-05-19 12:36:28,596 - INFO - Using device: cpu
2025-05-19 12:36:28,625 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-19 12:38:44,552 - INFO - Class weights: tensor([0.8034, 1.2247, 0.8137, 0.8838, 0.8771, 2.2498, 0.8771, 1.1203, 0.7191,
        0.8874, 0.8771, 0.9236, 0.8303, 0.9125])
2025-05-19 12:40:56,215 - INFO - 
Processing fold 1/5
2025-05-19 12:43:46,190 - INFO - Fold 1, Epoch 1/10, Train Loss: 0.3495, Val Loss: 0.2664
2025-05-19 12:46:34,084 - INFO - Fold 1, Epoch 2/10, Train Loss: 0.2478, Val Loss: 0.2995
2025-05-19 12:46:34,085 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 12:49:24,678 - INFO - Fold 1, Epoch 3/10, Train Loss: 0.2231, Val Loss: 0.2528
2025-05-19 12:52:16,214 - INFO - Fold 1, Epoch 4/10, Train Loss: 0.1993, Val Loss: 0.2145
2025-05-19 12:55:06,923 - INFO - Fold 1, Epoch 5/10, Train Loss: 0.1950, Val Loss: 0.2173
2025-05-19 12:55:06,924 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 12:57:57,510 - INFO - Fold 1, Epoch 6/10, Train Loss: 0.1797, Val Loss: 0.1952
2025-05-19 13:00:48,338 - INFO - Fold 1, Epoch 7/10, Train Loss: 0.1778, Val Loss: 0.1779
2025-05-19 13:03:38,980 - INFO - Fold 1, Epoch 8/10, Train Loss: 0.1632, Val Loss: 0.1971
2025-05-19 13:03:38,980 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 13:06:30,058 - INFO - Fold 1, Epoch 9/10, Train Loss: 0.1696, Val Loss: 0.1689
2025-05-19 13:09:20,155 - INFO - Fold 1, Epoch 10/10, Train Loss: 0.1609, Val Loss: 0.1864
2025-05-19 13:09:20,155 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 13:09:47,873 - INFO - 
Processing fold 2/5
2025-05-19 13:12:34,933 - INFO - Fold 2, Epoch 1/10, Train Loss: 0.3645, Val Loss: 0.2701
2025-05-19 13:15:20,118 - INFO - Fold 2, Epoch 2/10, Train Loss: 0.2491, Val Loss: 0.3089
2025-05-19 13:15:20,118 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 13:18:05,980 - INFO - Fold 2, Epoch 3/10, Train Loss: 0.2098, Val Loss: 0.1938
2025-05-19 13:20:52,229 - INFO - Fold 2, Epoch 4/10, Train Loss: 0.1906, Val Loss: 0.2513
2025-05-19 13:20:52,229 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 13:23:38,661 - INFO - Fold 2, Epoch 5/10, Train Loss: 0.1875, Val Loss: 0.2041
2025-05-19 13:23:38,661 - INFO - EarlyStopping counter: 2 out of 3
2025-05-19 13:26:24,601 - INFO - Fold 2, Epoch 6/10, Train Loss: 0.1790, Val Loss: 0.1978
2025-05-19 13:26:24,601 - INFO - Learning rate reduced from 0.001000 to 0.000500
2025-05-19 13:26:24,601 - INFO - EarlyStopping counter: 3 out of 3
2025-05-19 13:26:24,602 - INFO - Early stopping triggered at epoch 6
2025-05-19 13:26:52,108 - INFO - 
Processing fold 3/5
2025-05-19 13:29:26,537 - INFO - Fold 3, Epoch 1/10, Train Loss: 0.3459, Val Loss: 0.2892
2025-05-19 13:32:01,445 - INFO - Fold 3, Epoch 2/10, Train Loss: 0.2363, Val Loss: 0.2098
2025-05-19 13:34:34,871 - INFO - Fold 3, Epoch 3/10, Train Loss: 0.2148, Val Loss: 0.2175
2025-05-19 13:34:34,871 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 13:37:08,585 - INFO - Fold 3, Epoch 4/10, Train Loss: 0.1948, Val Loss: 0.1911
2025-05-19 13:39:41,141 - INFO - Fold 3, Epoch 5/10, Train Loss: 0.1859, Val Loss: 0.3382
2025-05-19 13:39:41,141 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 13:42:14,905 - INFO - Fold 3, Epoch 6/10, Train Loss: 0.1793, Val Loss: 0.1997
2025-05-19 13:42:14,905 - INFO - EarlyStopping counter: 2 out of 3
2025-05-19 13:44:50,537 - INFO - Fold 3, Epoch 7/10, Train Loss: 0.1638, Val Loss: 0.1734
2025-05-19 13:47:23,823 - INFO - Fold 3, Epoch 8/10, Train Loss: 0.1664, Val Loss: 0.1943
2025-05-19 13:47:23,823 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 13:49:58,255 - INFO - Fold 3, Epoch 9/10, Train Loss: 0.1585, Val Loss: 0.1716
2025-05-19 13:52:32,860 - INFO - Fold 3, Epoch 10/10, Train Loss: 0.1661, Val Loss: 0.1837
2025-05-19 13:52:32,861 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 13:52:58,072 - INFO - 
Processing fold 4/5
2025-05-19 13:55:45,340 - INFO - Fold 4, Epoch 1/10, Train Loss: 0.3620, Val Loss: 0.3554
2025-05-19 13:58:32,631 - INFO - Fold 4, Epoch 2/10, Train Loss: 0.2462, Val Loss: 0.2286
2025-05-19 14:01:19,464 - INFO - Fold 4, Epoch 3/10, Train Loss: 0.2148, Val Loss: 0.1831
2025-05-19 14:04:05,740 - INFO - Fold 4, Epoch 4/10, Train Loss: 0.2132, Val Loss: 0.1937
2025-05-19 14:04:05,740 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 14:06:52,241 - INFO - Fold 4, Epoch 5/10, Train Loss: 0.1823, Val Loss: 0.2106
2025-05-19 14:06:52,241 - INFO - EarlyStopping counter: 2 out of 3
2025-05-19 14:09:40,108 - INFO - Fold 4, Epoch 6/10, Train Loss: 0.1850, Val Loss: 0.1895
2025-05-19 14:09:40,108 - INFO - Learning rate reduced from 0.001000 to 0.000500
2025-05-19 14:09:40,108 - INFO - EarlyStopping counter: 3 out of 3
2025-05-19 14:09:40,108 - INFO - Early stopping triggered at epoch 6
2025-05-19 14:10:06,835 - INFO - 
Processing fold 5/5
2025-05-19 14:12:48,044 - INFO - Fold 5, Epoch 1/10, Train Loss: 0.3465, Val Loss: 0.2601
2025-05-19 14:15:29,042 - INFO - Fold 5, Epoch 2/10, Train Loss: 0.2360, Val Loss: 0.3089
2025-05-19 14:15:29,042 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 14:18:10,224 - INFO - Fold 5, Epoch 3/10, Train Loss: 0.2144, Val Loss: 0.2171
2025-05-19 14:20:51,470 - INFO - Fold 5, Epoch 4/10, Train Loss: 0.2091, Val Loss: 0.2271
2025-05-19 14:20:51,470 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 14:23:33,114 - INFO - Fold 5, Epoch 5/10, Train Loss: 0.1865, Val Loss: 0.2130
2025-05-19 14:26:13,848 - INFO - Fold 5, Epoch 6/10, Train Loss: 0.1706, Val Loss: 0.2042
2025-05-19 14:28:55,017 - INFO - Fold 5, Epoch 7/10, Train Loss: 0.1686, Val Loss: 0.1920
2025-05-19 14:31:35,992 - INFO - Fold 5, Epoch 8/10, Train Loss: 0.1648, Val Loss: 0.2066
2025-05-19 14:31:35,992 - INFO - EarlyStopping counter: 1 out of 3
2025-05-19 14:34:17,160 - INFO - Fold 5, Epoch 9/10, Train Loss: 0.1713, Val Loss: 0.2390
2025-05-19 14:34:17,160 - INFO - EarlyStopping counter: 2 out of 3
2025-05-19 14:36:58,977 - INFO - Fold 5, Epoch 10/10, Train Loss: 0.1528, Val Loss: 0.2053
2025-05-19 14:36:58,977 - INFO - Learning rate reduced from 0.001000 to 0.000500
2025-05-19 14:36:58,977 - INFO - EarlyStopping counter: 3 out of 3
2025-05-19 14:36:58,977 - INFO - Early stopping triggered at epoch 10
2025-05-19 14:37:25,707 - INFO - 
Evaluation completed successfully!
2025-05-25 17:07:16,679 - INFO - Using device: cpu
2025-05-25 17:07:16,708 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-25 17:09:05,026 - INFO - Class weights: tensor([0.8034, 1.2247, 0.8137, 0.8838, 0.8771, 2.2498, 0.8771, 1.1203, 0.7191,
        0.8874, 0.8771, 0.9236, 0.8303, 0.9125])
2025-05-25 17:10:49,546 - INFO - 
Processing fold 1/5
2025-05-25 17:13:46,575 - INFO - Fold 1, Epoch 1/15, Train Loss: 0.3234, Val Loss: 0.2018
2025-05-25 17:16:38,025 - INFO - Fold 1, Epoch 2/15, Train Loss: 0.1778, Val Loss: 0.1980
2025-05-25 17:19:33,089 - INFO - Fold 1, Epoch 3/15, Train Loss: 0.1513, Val Loss: 0.1674
2025-05-25 17:22:18,299 - INFO - Fold 1, Epoch 4/15, Train Loss: 0.1357, Val Loss: 0.1698
2025-05-25 17:22:18,299 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 17:25:09,188 - INFO - Fold 1, Epoch 5/15, Train Loss: 0.1231, Val Loss: 0.1641
2025-05-25 17:28:00,779 - INFO - Fold 1, Epoch 6/15, Train Loss: 0.1187, Val Loss: 0.1666
2025-05-25 17:28:00,779 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 17:30:53,516 - INFO - Fold 1, Epoch 7/15, Train Loss: 0.1106, Val Loss: 0.1616
2025-05-25 17:33:44,008 - INFO - Fold 1, Epoch 8/15, Train Loss: 0.1045, Val Loss: 0.1562
2025-05-25 17:36:33,146 - INFO - Fold 1, Epoch 9/15, Train Loss: 0.1012, Val Loss: 0.1528
2025-05-25 17:39:21,868 - INFO - Fold 1, Epoch 10/15, Train Loss: 0.0952, Val Loss: 0.1467
2025-05-25 17:42:16,459 - INFO - Fold 1, Epoch 11/15, Train Loss: 0.0938, Val Loss: 0.1652
2025-05-25 17:42:16,459 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 17:45:17,293 - INFO - Fold 1, Epoch 12/15, Train Loss: 0.0874, Val Loss: 0.1302
2025-05-25 17:48:17,207 - INFO - Fold 1, Epoch 13/15, Train Loss: 0.0875, Val Loss: 0.1514
2025-05-25 17:48:17,208 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 17:51:13,502 - INFO - Fold 1, Epoch 14/15, Train Loss: 0.0831, Val Loss: 0.1583
2025-05-25 17:51:13,502 - INFO - EarlyStopping counter: 2 out of 3
2025-05-25 17:54:07,087 - INFO - Fold 1, Epoch 15/15, Train Loss: 0.0789, Val Loss: 0.1546
2025-05-25 17:54:07,087 - INFO - Learning rate reduced from 0.000100 to 0.000050
2025-05-25 17:54:07,087 - INFO - EarlyStopping counter: 3 out of 3
2025-05-25 17:54:07,087 - INFO - Early stopping triggered at epoch 15
2025-05-25 17:57:37,235 - INFO - t-SNE visualization saved to outputs/tsne_rice_classes.png
2025-05-25 17:57:37,235 - INFO - 
Processing fold 2/5
2025-05-25 18:00:27,847 - INFO - Fold 2, Epoch 1/15, Train Loss: 0.3327, Val Loss: 0.2228
2025-05-25 18:03:16,159 - INFO - Fold 2, Epoch 2/15, Train Loss: 0.1802, Val Loss: 0.1794
2025-05-25 18:06:01,442 - INFO - Fold 2, Epoch 3/15, Train Loss: 0.1462, Val Loss: 0.1670
2025-05-25 18:08:46,017 - INFO - Fold 2, Epoch 4/15, Train Loss: 0.1368, Val Loss: 0.1772
2025-05-25 18:08:46,017 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 18:11:31,639 - INFO - Fold 2, Epoch 5/15, Train Loss: 0.1289, Val Loss: 0.1679
2025-05-25 18:11:31,639 - INFO - EarlyStopping counter: 2 out of 3
2025-05-25 18:14:16,364 - INFO - Fold 2, Epoch 6/15, Train Loss: 0.1160, Val Loss: 0.1605
2025-05-25 18:17:02,107 - INFO - Fold 2, Epoch 7/15, Train Loss: 0.1090, Val Loss: 0.1485
2025-05-25 18:19:47,898 - INFO - Fold 2, Epoch 8/15, Train Loss: 0.1027, Val Loss: 0.1999
2025-05-25 18:19:47,898 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 18:22:33,843 - INFO - Fold 2, Epoch 9/15, Train Loss: 0.0988, Val Loss: 0.1669
2025-05-25 18:22:33,843 - INFO - EarlyStopping counter: 2 out of 3
2025-05-25 18:25:18,509 - INFO - Fold 2, Epoch 10/15, Train Loss: 0.0992, Val Loss: 0.1967
2025-05-25 18:25:18,509 - INFO - Learning rate reduced from 0.000100 to 0.000050
2025-05-25 18:25:18,509 - INFO - EarlyStopping counter: 3 out of 3
2025-05-25 18:25:18,509 - INFO - Early stopping triggered at epoch 10
2025-05-25 18:25:46,185 - INFO - 
Processing fold 3/5
2025-05-25 18:28:18,776 - INFO - Fold 3, Epoch 1/15, Train Loss: 0.3287, Val Loss: 0.2429
2025-05-25 18:30:50,445 - INFO - Fold 3, Epoch 2/15, Train Loss: 0.1691, Val Loss: 0.2120
2025-05-25 18:33:22,049 - INFO - Fold 3, Epoch 3/15, Train Loss: 0.1548, Val Loss: 0.1813
2025-05-25 18:35:54,138 - INFO - Fold 3, Epoch 4/15, Train Loss: 0.1372, Val Loss: 0.1690
2025-05-25 18:38:26,738 - INFO - Fold 3, Epoch 5/15, Train Loss: 0.1218, Val Loss: 0.1799
2025-05-25 18:38:26,738 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 18:41:04,972 - INFO - Fold 3, Epoch 6/15, Train Loss: 0.1103, Val Loss: 0.1799
2025-05-25 18:41:04,972 - INFO - EarlyStopping counter: 2 out of 3
2025-05-25 18:43:43,080 - INFO - Fold 3, Epoch 7/15, Train Loss: 0.1092, Val Loss: 0.1648
2025-05-25 18:46:14,518 - INFO - Fold 3, Epoch 8/15, Train Loss: 0.1052, Val Loss: 0.1470
2025-05-25 18:48:46,571 - INFO - Fold 3, Epoch 9/15, Train Loss: 0.0993, Val Loss: 0.1562
2025-05-25 18:48:46,571 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 18:51:19,444 - INFO - Fold 3, Epoch 10/15, Train Loss: 0.0923, Val Loss: 0.1796
2025-05-25 18:51:19,444 - INFO - EarlyStopping counter: 2 out of 3
2025-05-25 18:53:52,023 - INFO - Fold 3, Epoch 11/15, Train Loss: 0.0932, Val Loss: 0.1795
2025-05-25 18:53:52,023 - INFO - Learning rate reduced from 0.000100 to 0.000050
2025-05-25 18:53:52,023 - INFO - EarlyStopping counter: 3 out of 3
2025-05-25 18:53:52,023 - INFO - Early stopping triggered at epoch 11
2025-05-25 18:54:18,657 - INFO - 
Processing fold 4/5
2025-05-25 18:56:56,512 - INFO - Fold 4, Epoch 1/15, Train Loss: 0.3343, Val Loss: 0.2126
2025-05-25 18:59:34,675 - INFO - Fold 4, Epoch 2/15, Train Loss: 0.1780, Val Loss: 0.2040
2025-05-25 19:02:12,153 - INFO - Fold 4, Epoch 3/15, Train Loss: 0.1541, Val Loss: 0.1575
2025-05-25 19:04:50,560 - INFO - Fold 4, Epoch 4/15, Train Loss: 0.1416, Val Loss: 0.1722
2025-05-25 19:04:50,561 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 19:07:28,092 - INFO - Fold 4, Epoch 5/15, Train Loss: 0.1274, Val Loss: 0.1541
2025-05-25 19:10:06,470 - INFO - Fold 4, Epoch 6/15, Train Loss: 0.1142, Val Loss: 0.1551
2025-05-25 19:10:06,470 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 19:12:43,983 - INFO - Fold 4, Epoch 7/15, Train Loss: 0.1127, Val Loss: 0.1516
2025-05-25 19:15:21,811 - INFO - Fold 4, Epoch 8/15, Train Loss: 0.1087, Val Loss: 0.1444
2025-05-25 19:18:00,311 - INFO - Fold 4, Epoch 9/15, Train Loss: 0.0969, Val Loss: 0.1465
2025-05-25 19:18:00,311 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 19:20:38,260 - INFO - Fold 4, Epoch 10/15, Train Loss: 0.0947, Val Loss: 0.1384
2025-05-25 19:23:16,685 - INFO - Fold 4, Epoch 11/15, Train Loss: 0.0934, Val Loss: 0.1415
2025-05-25 19:23:16,685 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 19:25:54,717 - INFO - Fold 4, Epoch 12/15, Train Loss: 0.0893, Val Loss: 0.1430
2025-05-25 19:25:54,718 - INFO - EarlyStopping counter: 2 out of 3
2025-05-25 19:28:33,569 - INFO - Fold 4, Epoch 13/15, Train Loss: 0.0841, Val Loss: 0.1387
2025-05-25 19:28:33,569 - INFO - Learning rate reduced from 0.000100 to 0.000050
2025-05-25 19:28:33,569 - INFO - EarlyStopping counter: 3 out of 3
2025-05-25 19:28:33,569 - INFO - Early stopping triggered at epoch 13
2025-05-25 19:29:00,147 - INFO - 
Processing fold 5/5
2025-05-25 19:31:38,381 - INFO - Fold 5, Epoch 1/15, Train Loss: 0.3218, Val Loss: 0.2052
2025-05-25 19:34:15,144 - INFO - Fold 5, Epoch 2/15, Train Loss: 0.1741, Val Loss: 0.2035
2025-05-25 19:36:52,539 - INFO - Fold 5, Epoch 3/15, Train Loss: 0.1515, Val Loss: 0.1726
2025-05-25 19:39:30,641 - INFO - Fold 5, Epoch 4/15, Train Loss: 0.1320, Val Loss: 0.2028
2025-05-25 19:39:30,641 - INFO - EarlyStopping counter: 1 out of 3
2025-05-25 19:42:08,795 - INFO - Fold 5, Epoch 5/15, Train Loss: 0.1211, Val Loss: 0.1861
2025-05-25 19:42:08,795 - INFO - EarlyStopping counter: 2 out of 3
2025-05-25 19:44:47,651 - INFO - Fold 5, Epoch 6/15, Train Loss: 0.1161, Val Loss: 0.1854
2025-05-25 19:44:47,651 - INFO - Learning rate reduced from 0.000100 to 0.000050
2025-05-25 19:44:47,651 - INFO - EarlyStopping counter: 3 out of 3
2025-05-25 19:44:47,651 - INFO - Early stopping triggered at epoch 6
2025-05-25 19:45:14,777 - INFO - 
Evaluation completed successfully!
2025-05-30 16:54:38,629 - INFO - Using device: cpu
2025-05-30 16:54:38,661 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-30 16:55:10,754 - INFO - Using device: cpu
2025-05-30 16:55:10,783 - INFO - Loaded datasets from /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/dataset_organized with 13024 samples
2025-05-30 16:57:03,356 - INFO - Class weights: tensor([0.8034, 1.2247, 0.8137, 0.8838, 0.8771, 2.2498, 0.8771, 1.1203, 0.7191,
        0.8874, 0.8771, 0.9236, 0.8303, 0.9125])
2025-05-30 16:58:41,887 - INFO - 
Processing fold 1/5
2025-05-30 17:01:37,058 - INFO - Fold 1, Epoch 1/15, Train Loss: 0.3234, Val Loss: 0.2018
2025-05-30 17:04:29,513 - INFO - Fold 1, Epoch 2/15, Train Loss: 0.1778, Val Loss: 0.1980
2025-05-30 17:07:23,044 - INFO - Fold 1, Epoch 3/15, Train Loss: 0.1513, Val Loss: 0.1674
2025-05-30 17:10:09,988 - INFO - Fold 1, Epoch 4/15, Train Loss: 0.1357, Val Loss: 0.1698
2025-05-30 17:10:09,988 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 17:12:57,082 - INFO - Fold 1, Epoch 5/15, Train Loss: 0.1231, Val Loss: 0.1641
2025-05-30 17:15:49,148 - INFO - Fold 1, Epoch 6/15, Train Loss: 0.1187, Val Loss: 0.1666
2025-05-30 17:15:49,148 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 17:18:38,059 - INFO - Fold 1, Epoch 7/15, Train Loss: 0.1106, Val Loss: 0.1616
2025-05-30 17:21:26,301 - INFO - Fold 1, Epoch 8/15, Train Loss: 0.1045, Val Loss: 0.1562
2025-05-30 17:24:15,308 - INFO - Fold 1, Epoch 9/15, Train Loss: 0.1012, Val Loss: 0.1528
2025-05-30 17:27:02,268 - INFO - Fold 1, Epoch 10/15, Train Loss: 0.0952, Val Loss: 0.1467
2025-05-30 17:29:51,935 - INFO - Fold 1, Epoch 11/15, Train Loss: 0.0938, Val Loss: 0.1652
2025-05-30 17:29:51,935 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 17:32:41,807 - INFO - Fold 1, Epoch 12/15, Train Loss: 0.0874, Val Loss: 0.1302
2025-05-30 17:35:30,118 - INFO - Fold 1, Epoch 13/15, Train Loss: 0.0875, Val Loss: 0.1514
2025-05-30 17:35:30,118 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 17:38:17,856 - INFO - Fold 1, Epoch 14/15, Train Loss: 0.0831, Val Loss: 0.1583
2025-05-30 17:38:17,856 - INFO - EarlyStopping counter: 2 out of 3
2025-05-30 17:41:06,420 - INFO - Fold 1, Epoch 15/15, Train Loss: 0.0789, Val Loss: 0.1546
2025-05-30 17:41:06,420 - INFO - Learning rate reduced from 0.000100 to 0.000050
2025-05-30 17:41:06,420 - INFO - EarlyStopping counter: 3 out of 3
2025-05-30 17:41:06,420 - INFO - Early stopping triggered at epoch 15
2025-05-30 17:44:26,895 - INFO - t-SNE visualization saved to outputs/tsne_rice_classes.png
2025-05-30 17:44:26,896 - INFO - New overall best model found in fold 1 with validation loss: 0.1302
2025-05-30 17:44:26,896 - INFO - 
Processing fold 2/5
2025-05-30 17:47:10,455 - INFO - Fold 2, Epoch 1/15, Train Loss: 0.3327, Val Loss: 0.2228
2025-05-30 17:49:55,364 - INFO - Fold 2, Epoch 2/15, Train Loss: 0.1802, Val Loss: 0.1794
2025-05-30 17:52:36,993 - INFO - Fold 2, Epoch 3/15, Train Loss: 0.1462, Val Loss: 0.1670
2025-05-30 17:55:17,662 - INFO - Fold 2, Epoch 4/15, Train Loss: 0.1368, Val Loss: 0.1772
2025-05-30 17:55:17,662 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 17:57:58,252 - INFO - Fold 2, Epoch 5/15, Train Loss: 0.1289, Val Loss: 0.1679
2025-05-30 17:57:58,252 - INFO - EarlyStopping counter: 2 out of 3
2025-05-30 18:00:37,708 - INFO - Fold 2, Epoch 6/15, Train Loss: 0.1160, Val Loss: 0.1605
2025-05-30 18:03:17,782 - INFO - Fold 2, Epoch 7/15, Train Loss: 0.1090, Val Loss: 0.1485
2025-05-30 18:05:57,743 - INFO - Fold 2, Epoch 8/15, Train Loss: 0.1027, Val Loss: 0.1999
2025-05-30 18:05:57,743 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 18:08:38,568 - INFO - Fold 2, Epoch 9/15, Train Loss: 0.0988, Val Loss: 0.1669
2025-05-30 18:08:38,568 - INFO - EarlyStopping counter: 2 out of 3
2025-05-30 18:11:18,301 - INFO - Fold 2, Epoch 10/15, Train Loss: 0.0992, Val Loss: 0.1967
2025-05-30 18:11:18,301 - INFO - Learning rate reduced from 0.000100 to 0.000050
2025-05-30 18:11:18,301 - INFO - EarlyStopping counter: 3 out of 3
2025-05-30 18:11:18,301 - INFO - Early stopping triggered at epoch 10
2025-05-30 18:11:45,141 - INFO - 
Processing fold 3/5
2025-05-30 18:14:23,533 - INFO - Fold 3, Epoch 1/15, Train Loss: 0.3287, Val Loss: 0.2429
2025-05-30 18:17:01,875 - INFO - Fold 3, Epoch 2/15, Train Loss: 0.1691, Val Loss: 0.2120
2025-05-30 18:19:41,658 - INFO - Fold 3, Epoch 3/15, Train Loss: 0.1548, Val Loss: 0.1813
2025-05-30 18:22:20,307 - INFO - Fold 3, Epoch 4/15, Train Loss: 0.1372, Val Loss: 0.1690
2025-05-30 18:24:59,035 - INFO - Fold 3, Epoch 5/15, Train Loss: 0.1218, Val Loss: 0.1799
2025-05-30 18:24:59,035 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 18:27:37,455 - INFO - Fold 3, Epoch 6/15, Train Loss: 0.1103, Val Loss: 0.1799
2025-05-30 18:27:37,456 - INFO - EarlyStopping counter: 2 out of 3
2025-05-30 18:30:15,843 - INFO - Fold 3, Epoch 7/15, Train Loss: 0.1092, Val Loss: 0.1648
2025-05-30 18:32:55,570 - INFO - Fold 3, Epoch 8/15, Train Loss: 0.1052, Val Loss: 0.1470
2025-05-30 18:35:33,440 - INFO - Fold 3, Epoch 9/15, Train Loss: 0.0993, Val Loss: 0.1562
2025-05-30 18:35:33,440 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 18:38:11,593 - INFO - Fold 3, Epoch 10/15, Train Loss: 0.0923, Val Loss: 0.1796
2025-05-30 18:38:11,593 - INFO - EarlyStopping counter: 2 out of 3
2025-05-30 18:40:49,763 - INFO - Fold 3, Epoch 11/15, Train Loss: 0.0932, Val Loss: 0.1795
2025-05-30 18:40:49,763 - INFO - Learning rate reduced from 0.000100 to 0.000050
2025-05-30 18:40:49,763 - INFO - EarlyStopping counter: 3 out of 3
2025-05-30 18:40:49,763 - INFO - Early stopping triggered at epoch 11
2025-05-30 18:41:15,872 - INFO - 
Processing fold 4/5
2025-05-30 18:44:03,097 - INFO - Fold 4, Epoch 1/15, Train Loss: 0.3343, Val Loss: 0.2126
2025-05-30 18:46:52,801 - INFO - Fold 4, Epoch 2/15, Train Loss: 0.1780, Val Loss: 0.2040
2025-05-30 18:49:41,497 - INFO - Fold 4, Epoch 3/15, Train Loss: 0.1541, Val Loss: 0.1575
2025-05-30 18:52:31,237 - INFO - Fold 4, Epoch 4/15, Train Loss: 0.1416, Val Loss: 0.1722
2025-05-30 18:52:31,237 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 18:55:20,324 - INFO - Fold 4, Epoch 5/15, Train Loss: 0.1274, Val Loss: 0.1541
2025-05-30 18:58:09,848 - INFO - Fold 4, Epoch 6/15, Train Loss: 0.1142, Val Loss: 0.1551
2025-05-30 18:58:09,848 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 19:00:59,678 - INFO - Fold 4, Epoch 7/15, Train Loss: 0.1127, Val Loss: 0.1516
2025-05-30 19:03:49,390 - INFO - Fold 4, Epoch 8/15, Train Loss: 0.1087, Val Loss: 0.1444
2025-05-30 19:06:38,278 - INFO - Fold 4, Epoch 9/15, Train Loss: 0.0969, Val Loss: 0.1465
2025-05-30 19:06:38,278 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 19:09:28,000 - INFO - Fold 4, Epoch 10/15, Train Loss: 0.0947, Val Loss: 0.1384
2025-05-30 19:12:18,654 - INFO - Fold 4, Epoch 11/15, Train Loss: 0.0934, Val Loss: 0.1415
2025-05-30 19:12:18,654 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 19:15:08,156 - INFO - Fold 4, Epoch 12/15, Train Loss: 0.0893, Val Loss: 0.1430
2025-05-30 19:15:08,156 - INFO - EarlyStopping counter: 2 out of 3
2025-05-30 19:17:58,310 - INFO - Fold 4, Epoch 13/15, Train Loss: 0.0841, Val Loss: 0.1387
2025-05-30 19:17:58,310 - INFO - Learning rate reduced from 0.000100 to 0.000050
2025-05-30 19:17:58,311 - INFO - EarlyStopping counter: 3 out of 3
2025-05-30 19:17:58,311 - INFO - Early stopping triggered at epoch 13
2025-05-30 19:18:26,148 - INFO - 
Processing fold 5/5
2025-05-30 19:21:06,979 - INFO - Fold 5, Epoch 1/15, Train Loss: 0.3218, Val Loss: 0.2052
2025-05-30 19:23:50,666 - INFO - Fold 5, Epoch 2/15, Train Loss: 0.1741, Val Loss: 0.2035
2025-05-30 19:26:34,968 - INFO - Fold 5, Epoch 3/15, Train Loss: 0.1515, Val Loss: 0.1726
2025-05-30 19:29:18,254 - INFO - Fold 5, Epoch 4/15, Train Loss: 0.1320, Val Loss: 0.2028
2025-05-30 19:29:18,254 - INFO - EarlyStopping counter: 1 out of 3
2025-05-30 19:32:02,884 - INFO - Fold 5, Epoch 5/15, Train Loss: 0.1211, Val Loss: 0.1861
2025-05-30 19:32:02,884 - INFO - EarlyStopping counter: 2 out of 3
2025-05-30 19:34:46,995 - INFO - Fold 5, Epoch 6/15, Train Loss: 0.1161, Val Loss: 0.1854
2025-05-30 19:34:46,996 - INFO - Learning rate reduced from 0.000100 to 0.000050
2025-05-30 19:34:46,996 - INFO - EarlyStopping counter: 3 out of 3
2025-05-30 19:34:46,996 - INFO - Early stopping triggered at epoch 6
2025-05-30 19:35:15,484 - INFO - Overall best model saved to /home/yan/Documents/Git/SDS-CP028-smart-leaf/submissions/team-members/yan-cotta/scripts/outputs/best_leaf_disease_model.pth
2025-05-30 19:35:15,484 - INFO - 
Evaluation completed successfully!
