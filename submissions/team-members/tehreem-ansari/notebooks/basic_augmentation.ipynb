{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87dad7e1-f32b-464c-817c-97250b6aaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import ipynbname\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d97a7dd-f053-4c57-9116-4fcc8f26ee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_augmentation\n"
     ]
    }
   ],
   "source": [
    "print(ipynbname.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a890172c-9e32-49dc-8da8-e53ae29a008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base data directory\n",
    "base_dir = Path('/Users/tehreem/Desktop/Study/Projects/SDS-CP028-smart-leaf/submissions/team-members/tehreem-ansari/data')\n",
    "source_dir = base_dir / 'ValidCrops'\n",
    "target_dirs = {\n",
    "    'train': base_dir /ipynbname.name() / 'train',\n",
    "    'val': base_dir/ipynbname.name() / 'val',\n",
    "    'test': base_dir/ipynbname.name() / 'test'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f509628e-d2a1-4634-9062-2d0f4ce72d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown file format ignoring: .DS_Store\n",
      "Unknown file format ignoring: .DS_Store\n",
      "Unknown file format ignoring: .DS_Store\n",
      "Unknown file format ignoring: DOC-20231219-WA0001.pdf\n",
      "Total corrupt images deleted: 0\n"
     ]
    }
   ],
   "source": [
    "#First remove corrupt images before splitting your data into train, validation, and test sets.\n",
    "#If a corrupt image ends up in any split, it can cause your training or evaluation code to crash or produce errors.\n",
    "#If corrupt images are removed after splitting, some splits may lose more images than others, leading to imbalanced or unrepresentative datasets\n",
    "\n",
    "#Remove Corrupt Images\n",
    "def remove_corrupt_images(directory):\n",
    "    corrupted_files = []\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            normalized_name = filename.strip().lower()\n",
    "            if normalized_name.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.gif')):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        img.verify()\n",
    "                except Exception as e:\n",
    "                    corrupted_files.append(file_path)\n",
    "                    print(f\"Corrupt image found and deleted: {file_path} ({e})\")\n",
    "                    os.remove(file_path)\n",
    "            else:\n",
    "                print(f\"Unknown file format ignoring: {filename}\")\n",
    "    print(f\"Total corrupt images deleted: {len(corrupted_files)}\")\n",
    "\n",
    "\n",
    "\n",
    "remove_corrupt_images(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfd64833-f795-4f1c-9aef-63cbaf492e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split and copied successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create target directories for train, validation, and test splits if they don't exist\n",
    "for split_dir in target_dirs.values():\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Return a list of image file paths in the given directory.\n",
    "    Only files with extensions .jpg, .jpeg, .png (case-insensitive) are included.\n",
    "    \"\"\"\n",
    "def get_images(path: Path) -> list:\n",
    "    return [file for file in path.iterdir() if file.is_file() and file.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each crop category folder (e.g., corn, rice)\n",
    "for crop_folder in source_dir.iterdir():\n",
    "    if crop_folder.is_dir():\n",
    "        # Iterate over each disease subfolder within the crop folder\n",
    "        for disease_folder in crop_folder.iterdir():\n",
    "            if disease_folder.is_dir():\n",
    "                images = get_images(disease_folder)  # Get all images in this disease folder\n",
    "                random.shuffle(images)  # Shuffle images randomly before splitting\n",
    "\n",
    "                total = len(images)\n",
    "                train_split = int(0.8 * total)  # 80% for training\n",
    "                val_split = int(0.9 * total)    # Next 10% for validation (80% + 10%)\n",
    "\n",
    "                # Split images into train, validation, and test sets\n",
    "                train_images = images[:train_split]          # First 80%\n",
    "                val_images = images[train_split:val_split]   # Next 10%\n",
    "                test_images = images[val_split:]              # Remaining 10%\n",
    "\n",
    "                # Organize splits in a dictionary for easy iteration\n",
    "                split_data = {\n",
    "                    'train': train_images,\n",
    "                    'val': val_images,\n",
    "                    'test': test_images\n",
    "                }\n",
    "\n",
    "                class_name = disease_folder.name  # Use disease folder name as class label\n",
    "\n",
    "                # For each split, copy images to corresponding target directory/class folder\n",
    "                for split, image_list in split_data.items():\n",
    "                    class_dir = target_dirs[split] / class_name  # e.g., train/corn_early_blight\n",
    "                    os.makedirs(class_dir, exist_ok=True)        # Create class folder if it doesn't exist\n",
    "                    for image_path in image_list:\n",
    "                        shutil.copy(image_path, class_dir / image_path.name)  # Copy image file\n",
    "\n",
    "print(\"Data split and copied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb446d63-ecd9-4f1b-9f1f-bea9ffefdf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting from directory instead of dataset, this is fast\n",
    "def count_images_in_train(train_dir):\n",
    "    image_counts = {}\n",
    "    # Traverse through all class directories in train\n",
    "    for class_dir in train_dir.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            # Count the images in each class folder\n",
    "            image_count = len([file for file in class_dir.iterdir() if file.is_file() and file.suffix.lower() in ['.jpg', '.jpeg', '.png']])\n",
    "            image_counts[class_dir.name] = image_count\n",
    "    \n",
    "    return image_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0038d56-319d-4f56-9d45-c3a48c01da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image count for each class in train set\n",
    "class_counts = count_images_in_train(target_dirs['train'])\n",
    "\n",
    "# Print the image count for each class\n",
    "# for class_name, count in image_counts.items():\n",
    "def print_count_of_classes(class_counts):\n",
    "    plt.bar(class_counts.keys(), class_counts.values())\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Number of Images per Class\")\n",
    "    plt.show()\n",
    "    \n",
    "#print_count_of_classes(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc721e0d-afd9-48c6-98ec-2109ce6d8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists the classes and total images in each class\n",
    "def list_class_count(class_counts):\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"{class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc348365-8c49-42ac-a54f-e2a822a6d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10414 files belonging to 14 classes.\n",
      "Found 1301 files belonging to 14 classes.\n",
      "Found 1309 files belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "#Create the dataframes from directories\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)  # standard size for CNNs\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=target_dirs['train'],\n",
    "    labels=\"inferred\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=target_dirs['val'],\n",
    "    labels=\"inferred\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=target_dirs['test'],\n",
    "    labels=\"inferred\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names #Need to save class_names before doing normalization since dataset changes from _PrefetchDataset to MapDataset and mapdataset doesnt have class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5965aac1-f45a-4f6d-9893-1333d2a1066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheat___Brown_Rust: 721 images\n",
      "Potato___Early_Blight: 800 images\n",
      "Wheat___Healthy: 892 images\n",
      "Potato___Late_Blight: 800 images\n",
      "Wheat___Yellow_Rust: 739 images\n",
      "Rice___Healthy: 1190 images\n",
      "Corn___Northern_Leaf_Blight: 788 images\n",
      "Rice___Brown_Spot: 490 images\n",
      "Rice___Leaf_Blast: 781 images\n",
      "Corn___Common_Rust: 953 images\n",
      "Corn___Healthy: 929 images\n",
      "Corn___Gray_Leaf_Spot: 410 images\n",
      "Rice___Neck_Blast: 800 images\n",
      "Potato___Healthy: 121 images\n",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "list_class_count(count_images_in_train(target_dirs['train']))\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43ffe92f-dca7-4934-9c92-6dac5d91dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel range: 0.0 - 1.0\n",
      "Example pixel: [0.8742297 0.8742297 0.9840337]\n"
     ]
    }
   ],
   "source": [
    "# Normalize pixel values to [0,1]\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "#Verifying if normalization has happened\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(\"Pixel range:\", tf.reduce_min(images).numpy(), \"-\", tf.reduce_max(images).numpy())\n",
    "    print(\"Example pixel:\", images[0, 0, 0].numpy())  # Top-left pixel of the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1dd7f9ee-0fc9-4424-a64a-31308dd4f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data augmentation pipeline\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,           # Randomly rotate images up to 30 degrees\n",
    "    width_shift_range=0.2,       # Randomly shift images horizontally by up to 20% of width\n",
    "    height_shift_range=0.2,      # Randomly shift images vertically by up to 20% of height\n",
    "    zoom_range=0.2,              # Randomly zoom in/out by up to 20%\n",
    "    shear_range=0.2,             # Shear transformations up to 20%\n",
    "    horizontal_flip=True,        # Randomly flip images horizontally\n",
    "    fill_mode='nearest'          # Fill in new pixels after transformations with nearest pixel values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33aeb663-5615-4b33-9dd6-0faa35e667fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images(source_dir, save_dir, datagen, target_count, number_of_copy, keep_original=False):\n",
    "\n",
    "    image_count = 0  # Counter for total images saved (original + augmented)\n",
    "    \n",
    "    image_files = [f for f in source_dir.iterdir()\n",
    "               if f.is_file() and f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        # Stop if we have reached the target number of images\n",
    "        if image_count >= target_count:\n",
    "            break\n",
    "\n",
    "        # Optionally save the original image before augmentation\n",
    "        if keep_original:\n",
    "            dest_file = save_dir / image_file.name\n",
    "            # Copy original image only if it doesn't already exist in save_dir\n",
    "            if not dest_file.exists():\n",
    "                shutil.copy(image_file, dest_file)\n",
    "                image_count += 1  # Increment count for saved original image\n",
    "                # Check again if target count reached after saving original\n",
    "                if image_count >= target_count:\n",
    "                    break\n",
    "\n",
    "        # Load the image from disk\n",
    "        img = load_img(image_file)\n",
    "        # Convert the image to a numpy array\n",
    "        x = img_to_array(img)\n",
    "        # Reshape array to add batch dimension: (1, height, width, channels)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "\n",
    "        # Generate augmented images from this original image\n",
    "        i = 0  # Counter for augmented images generated per original image\n",
    "        #datagen.flow() method automatically saves each generated augmented image to the folder save_dir\n",
    "        for batch in datagen.flow(\n",
    "            x,\n",
    "            batch_size=1,\n",
    "            save_to_dir=save_dir,       # Directory to save augmented images\n",
    "            save_prefix='aug',          # Prefix for saved filenames\n",
    "            save_format='JPG'           # File format for saved images\n",
    "        ):\n",
    "            i += 1\n",
    "            image_count += 1  # Increment total image count\n",
    "            # Stop if reached target count or generated required copies for this image\n",
    "            if image_count >= target_count: #or i >= number_of_copy:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df8af9d8-9a3f-4525-88fb-2f91910dea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheat___Brown_Rust: 721 images\n",
      "Potato___Early_Blight: 800 images\n",
      "Wheat___Healthy: 892 images\n",
      "Potato___Late_Blight: 800 images\n",
      "Wheat___Yellow_Rust: 739 images\n",
      "Rice___Healthy: 1190 images\n",
      "Corn___Northern_Leaf_Blight: 788 images\n",
      "Rice___Brown_Spot: 490 images\n",
      "Rice___Leaf_Blast: 781 images\n",
      "Corn___Common_Rust: 953 images\n",
      "Corn___Healthy: 929 images\n",
      "Corn___Gray_Leaf_Spot: 410 images\n",
      "Rice___Neck_Blast: 800 images\n",
      "Potato___Healthy: 820 images\n"
     ]
    }
   ],
   "source": [
    "#Use augmentation to increase number of samples in Potato___Healthy \n",
    "'''\n",
    "Original count:\n",
    "\n",
    "Wheat___Brown_Rust: 721 images\n",
    "Potato___Early_Blight: 800 images\n",
    "Wheat___Healthy: 892 images\n",
    "Potato___Late_Blight: 800 images\n",
    "Wheat___Yellow_Rust: 739 images\n",
    "Rice___Healthy: 1190 images\n",
    "Corn___Northern_Leaf_Blight: 788 images\n",
    "Rice___Brown_Spot: 490 images\n",
    "Rice___Leaf_Blast: 781 images\n",
    "Corn___Common_Rust: 953 images\n",
    "Corn___Healthy: 929 images\n",
    "Corn___Gray_Leaf_Spot: 410 images\n",
    "Rice___Neck_Blast: 800 images\n",
    "Potato___Healthy: 121 images\n",
    "'''\n",
    "potato_source_dir = target_dirs['train']/'Potato___Healthy'  # change to your dataset path\n",
    "potato_target_count = 6 * len(list(potato_source_dir.glob('*.JPG')))  # 6x the dataset\n",
    "\n",
    "augment_images(potato_source_dir, potato_source_dir, datagen, potato_target_count, 2, True)\n",
    "list_class_count(count_images_in_train(target_dirs['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a628ae42-4267-4a83-904d-b0c4a9b2516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheat___Brown_Rust: 721 images\n",
      "Potato___Early_Blight: 800 images\n",
      "Wheat___Healthy: 892 images\n",
      "Potato___Late_Blight: 800 images\n",
      "Wheat___Yellow_Rust: 739 images\n",
      "Rice___Healthy: 1190 images\n",
      "Corn___Northern_Leaf_Blight: 788 images\n",
      "Rice___Brown_Spot: 490 images\n",
      "Rice___Leaf_Blast: 781 images\n",
      "Corn___Common_Rust: 953 images\n",
      "Corn___Healthy: 929 images\n",
      "Corn___Gray_Leaf_Spot: 760 images\n",
      "Rice___Neck_Blast: 800 images\n",
      "Potato___Healthy: 820 images\n"
     ]
    }
   ],
   "source": [
    "Corn___Gray_Leaf_Spot_source_dir = target_dirs['train']/'Corn___Gray_Leaf_Spot'  # change to your dataset path\n",
    "Corn___Gray_Leaf_Spot_target_count = 1 * len(list(Corn___Gray_Leaf_Spot_source_dir.glob('*.JPG')))  # One set of aug, hence doubling the dataset\n",
    "#Some of the images are in jpg and not JPG, hence they are not considered\n",
    "augment_images(Corn___Gray_Leaf_Spot_source_dir, Corn___Gray_Leaf_Spot_source_dir, datagen, Corn___Gray_Leaf_Spot_target_count, 1, True)\n",
    "list_class_count(count_images_in_train(target_dirs['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a454df9-d8c8-481d-8715-448ffe60c090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheat___Brown_Rust: 721 images\n",
      "Potato___Early_Blight: 800 images\n",
      "Wheat___Healthy: 892 images\n",
      "Potato___Late_Blight: 800 images\n",
      "Wheat___Yellow_Rust: 739 images\n",
      "Rice___Healthy: 1190 images\n",
      "Corn___Northern_Leaf_Blight: 788 images\n",
      "Rice___Brown_Spot: 969 images\n",
      "Rice___Leaf_Blast: 781 images\n",
      "Corn___Common_Rust: 953 images\n",
      "Corn___Healthy: 929 images\n",
      "Corn___Gray_Leaf_Spot: 760 images\n",
      "Rice___Neck_Blast: 800 images\n",
      "Potato___Healthy: 820 images\n"
     ]
    }
   ],
   "source": [
    "Rice___Brown_Spot_source_dir = target_dirs['train']/'Rice___Brown_Spot'  # change to your dataset path\n",
    "original_count = len([f for f in Rice___Brown_Spot_source_dir.glob('*.jpg') \n",
    "                      if not f.name.startswith('aug')])  # one set of augmented images\n",
    "\n",
    "augment_images(Rice___Brown_Spot_source_dir, Rice___Brown_Spot_source_dir, datagen, original_count, 1, True)\n",
    "list_class_count(count_images_in_train(target_dirs['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b714204b-2775-4c90-8991-8c891b53cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11942 files belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "#now the data is having more or less same samples.\n",
    "#re reading the train_ds to have all the sampeles\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=target_dirs['train'],\n",
    "    labels=\"inferred\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19f227fe-2bdc-4562-beaa-331d3072b312",
   "metadata": {},
   "source": [
    "'''Define the CNN\n",
    "Define a model-building function with hyperparameters\n",
    "These are the hyperparameters from Conv2D: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D#args\n",
    "Hyperparamets available to adjust:\n",
    " 1. Number of neural units\n",
    " 2. Number of layers\n",
    " 3. DROPOUT_RATE\n",
    " 4. Activation functions\n",
    " 5. epochs\n",
    " 6. kernal_size\n",
    " 7. Batch size\n",
    " 9. learning rate\n",
    " 10. early stop patience\n",
    " \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee8e6053-d0d3-4581-bd6b-9c146bbd00a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21632</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">692,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">462</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m18,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21632\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │       \u001b[38;5;34m692,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │           \u001b[38;5;34m462\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">730,574</span> (2.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m730,574\u001b[0m (2.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">730,574</span> (2.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m730,574\u001b[0m (2.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Define the CNN\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(shape=IMG_SIZE + (3,)),\n",
    "    layers.Conv2D(32, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(DROPOUT_RATE), # and keep_prob=0.7\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95b4e3a4-88e3-4382-b895-820efb0813c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "37c8a514-a5af-4835-9429-0a5bc2212f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(ipynbname.name()+'.keras', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b43eeb58-001e-4905-9169-12972af116f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 313ms/step - accuracy: 0.4494 - loss: 1.7015 - val_accuracy: 0.1745 - val_loss: 2.5245\n",
      "Epoch 2/20\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 351ms/step - accuracy: 0.3899 - loss: 1.8638 - val_accuracy: 0.1653 - val_loss: 2.5618\n",
      "Epoch 3/20\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 342ms/step - accuracy: 0.4331 - loss: 1.6704 - val_accuracy: 0.1453 - val_loss: 2.8945\n",
      "Epoch 4/20\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 347ms/step - accuracy: 0.5436 - loss: 1.2772 - val_accuracy: 0.0945 - val_loss: 5.6712\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "#We have ~11900 training images, and a batch size of 32, then: ceil(~11900 / 32) = 374 \n",
    "#Tensorflow computes: steps_per_epoch = math.ceil(total_training_samples / batch_size)\n",
    "\n",
    "epochs = 20\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0944754a-d15f-40ff-9788-191bca7cf92f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
